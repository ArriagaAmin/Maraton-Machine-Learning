{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E_TestingRandomForests.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSMvBVeA8xbz"
      },
      "source": [
        "# **BOSQUES ALEATORIOS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS5wldhR81Wf"
      },
      "source": [
        "Ejecutar preferiblemente en Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72OpBTq385Xs"
      },
      "source": [
        "## **Implementacion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pruQKfmZ8_Dg"
      },
      "source": [
        "### **Arboles de Decisiones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsW3N0a280O-"
      },
      "source": [
        "# Implementacion generica de Arboles de Decisiones.\n",
        "from math import log, inf, sqrt\n",
        "from random import randint\n",
        "\n",
        "class Node:\n",
        "  def __init__(self, parent, X, Y, atr_types, default):\n",
        "    self.parent = parent\n",
        "\n",
        "    # Ejemplos del entrenamiento que pertenecen a este nodo.\n",
        "    self.X = X\n",
        "    # Etiquetas de los ejemplos.\n",
        "    self.Y = Y\n",
        "\n",
        "    # Tipos de atributos de los ejemplos.\n",
        "    self.atr_types = atr_types\n",
        "    # Moda de las etiquetas.\n",
        "    self.default = default\n",
        "\n",
        "    self.childs = []\n",
        "    # La i-esima condicion corresponde al i-esimo hijo.\n",
        "    self.cond = []\n",
        "    self.leaf = True\n",
        "    # Etiqueta que recibe el patron al alcanzar esta nodo en caso de ser hoja.\n",
        "    self.value = None\n",
        "\n",
        "class DecisionTree:\n",
        "  def __init__(self, X, Y, atr_types, atr_name, atr_avail):\n",
        "    # Ejemplos de entrenamiento.\n",
        "    self.X = X\n",
        "    # Etiquetas de los ejemplos.\n",
        "    self.Y = Y \n",
        "    # Tipos de atributos de los ejemplos de entrenamiento. Hay dos tipos:\n",
        "    # \"Catg\" -> Categorico\n",
        "    # \"Cont\" -> Continuo\n",
        "    self.atr_types = atr_types\n",
        "    # Nombres de los atributos de los ejemplos de entrenamiento.\n",
        "    self.atr_name = atr_name\n",
        "    # Atributos disponibles\n",
        "    self.atr_avail = atr_avail\n",
        "\n",
        "  def gini(self, *P):\n",
        "    \"\"\" Gini impurity.\"\"\"\n",
        "    return 1 - sum(p**2 for p in P)\n",
        "\n",
        "  def entropy(self, *P):\n",
        "    \"\"\" Entropy for measure of randomness.\"\"\"\n",
        "    r = 0\n",
        "    for p in P:\n",
        "      if p==1: return 0\n",
        "      elif p>0: r -= p*log(p,2)\n",
        "    return r \n",
        "\n",
        "  def mayoria(self, Y):\n",
        "    \"\"\" Retorna la moda de un arreglo de elementos unitarios, ejemplo:\n",
        "     [[0], [1], [0]] -> mayoria = 0\"\"\"\n",
        "    dic = {}\n",
        "    for y in Y:\n",
        "      if y[0] in dic: dic[y[0]] += 1\n",
        "      else: dic[y[0]] = 1\n",
        "\n",
        "    best = None\n",
        "    max_c = 0\n",
        "    for d in dic:\n",
        "      if dic[d] > max_c:\n",
        "        max_c = dic[d]\n",
        "        best = d \n",
        "    return d \n",
        "\n",
        "  def get_values(self, X, a):\n",
        "    \"\"\" Obtenemos los posibles valores de un determinado atributo.\"\"\"\n",
        "    n = len(X[0])\n",
        "    values = []\n",
        "    for x in X:\n",
        "      if not x[a] in values: values.append(x[a])\n",
        "    return values\n",
        "\n",
        "  def gain_catg(self, a, values, X, Y):\n",
        "    \"\"\" Calculamos la ganancia de un atributo categorico en especifico. \"\"\"\n",
        "    # Calculamos la probabilidad de aparicion de cada etiqueta.\n",
        "    N = len(Y)\n",
        "    dic = {}\n",
        "    for y in Y:\n",
        "      if y[0] in dic: dic[y[0]] += 1/N\n",
        "      else: dic[y[0]] = 1/N\n",
        "    # Calculamos la entropia del nodo actual.\n",
        "    r = self.crit(*[dic[d] for d in dic])\n",
        "\n",
        "    # Calculamos la entropia de cada nodo luego de la division\n",
        "    # y se lo restamos a la entropia del nodo actual.\n",
        "    # Por cada valor del atributo indicado.\n",
        "    for v in values:\n",
        "\n",
        "      # Calculamos la probabilidad de aparicion de cada etiqueta dado\n",
        "      # que el atributo indicado tiene el valor v.\n",
        "      dic = {}\n",
        "      N_i = 0\n",
        "      for i, y in enumerate(Y):\n",
        "        if y[0] in dic and X[i][a]==v: \n",
        "          dic[y[0]] += 1\n",
        "          N_i += 1\n",
        "        elif X[i][a]==v: \n",
        "          dic[y[0]] = 1\n",
        "          N_i += 1\n",
        "\n",
        "      # Calculamos la entropia de una de las divisiones.\n",
        "      r -= N_i*self.crit(*[dic[d]/N_i for d in dic])/N\n",
        "    return r\n",
        "\n",
        "  def gain_cont(self, a, values, X, Y):\n",
        "    \"\"\" Calculamos la ganancia de un atributo continuo en especifico. \"\"\"\n",
        "    # Calculamos la probabilidad de aparicion de cada etiqueta.\n",
        "    N = len(Y)\n",
        "    dic = {}\n",
        "    for y in Y:\n",
        "      if y[0] in dic: dic[y[0]] += 1/N\n",
        "      else: dic[y[0]] = 1/N\n",
        "    # Calculamos la entropia del nodo actual.\n",
        "    r = self.crit(*[dic[d] for d in dic])\n",
        "\n",
        "    # Obtenemos las posibles divisiones binarias\n",
        "    values.sort()\n",
        "    divs = [(values[i]+values[i+1])/2 for i in range(len(values)-1)]\n",
        "\n",
        "    # Elegimos la division con la entropia minima\n",
        "    min_e = inf\n",
        "    best_d = -1\n",
        "    for d in divs:\n",
        "      # Calculamos la probabilidad de aparicion de cada etiqueta dado\n",
        "      # que el atributo es mayor o igual a la division.\n",
        "      dic = {}\n",
        "      N_i = 0\n",
        "      for i, y in enumerate(Y):\n",
        "        if y[0] in dic and X[i][a]>=d: \n",
        "          dic[y[0]] += 1\n",
        "          N_i += 1\n",
        "        elif X[i][a]>=d: \n",
        "          dic[y[0]] = 1\n",
        "          N_i += 1\n",
        "\n",
        "      # Calculamos la entropia de una de las divisiones.\n",
        "      e = N_i*self.crit(*[dic[d]/N_i for d in dic])/N\n",
        "\n",
        "      # Calculamos la probabilidad de aparicion de cada etiqueta dado\n",
        "      # que el atributo es menor a la division.\n",
        "      dic = {}\n",
        "      N_i = 0\n",
        "      for i, y in enumerate(Y):\n",
        "        if y[0] in dic and X[i][a]<d: \n",
        "          dic[y[0]] += 1\n",
        "          N_i += 1\n",
        "        elif X[i][a]<d: \n",
        "          dic[y[0]] = 1\n",
        "          N_i += 1\n",
        "      # Calculamos la entropia de una de las divisiones.\n",
        "      e += N_i*self.crit(*[dic[d]/N_i for d in dic])/N\n",
        "\n",
        "      if e < min_e:\n",
        "        min_e = e \n",
        "        best_d = d\n",
        "    \n",
        "    # Retornamos la entropia actual menos la de las divisiones\n",
        "    return r - min_e, best_d\n",
        "\n",
        "  def train(self, splits = -1, criterio=\"Gini\"):\n",
        "    \"\"\" Entrenamos el arbol de decisiones segun el numero de divisiones\n",
        "    y el criterio de division.\"\"\"\n",
        "    if criterio == \"Entropy\": self.crit = self.entropy\n",
        "    elif criterio == \"Gini\": self.crit = self.gini\n",
        "\n",
        "    root = Node(None, self.X, self.Y, self.atr_types, self.mayoria(self.Y))\n",
        "    queue = [root]\n",
        "    self.tree = root\n",
        "    atr_avail = self.atr_avail\n",
        "\n",
        "    # Usaremos un BFS en vez de DFS.\n",
        "    while len(queue) > 0:\n",
        "      node = queue.pop(0)\n",
        "\n",
        "      # Si no hay mas ejemplos, tomamos el default del padre.\n",
        "      if len(node.X) == 0: node.value = node.parent.default\n",
        "      # Si todos los ejemplos tienen la misma etiqueta, entonces sesa etiqueta\n",
        "      # sera el valor del nodo.\n",
        "      elif all(node.Y[0] == y for y in node.Y): node.value = node.Y[0][0]\n",
        "      # Si los ejemplos no tienen mas atributos, tomamos la moda de las etiquetas.\n",
        "      elif all(atr == 0 for atr in atr_avail) or splits == 0: node.value = self.mayoria(node.Y)\n",
        "      # Si no, se realizara una division.\n",
        "      else:\n",
        "        node.leaf = False\n",
        "        splits -= 1\n",
        "\n",
        "        # Obtenemos el mejor atributo calculando la ganancia de informacion\n",
        "        # de cada uno de ellos.\n",
        "        best = -1\n",
        "        best_g = -1\n",
        "        div = -1\n",
        "        for a in range(len(node.X[0])):\n",
        "          if atr_avail[a] != 0:\n",
        "            values = self.get_values(node.X, a)\n",
        "            if node.atr_types[a] == \"Catg\": \n",
        "              g = self.gain_catg(a, values, node.X, node.Y)\n",
        "              if g > best_g:\n",
        "                best_g = g \n",
        "                best = a\n",
        "            else: \n",
        "              g, div = self.gain_cont(a, values, node.X, node.Y)\n",
        "              if g > best_g:\n",
        "                best_g = g \n",
        "                best = a\n",
        "                best_d = div\n",
        "      \n",
        "        # Verificamos si el mejor atributo es categorico o continuo.\n",
        "        if node.atr_types[best] == \"Catg\":\n",
        "          atr_avail[best] = 0\n",
        "          # Particionamos los ejemplos segun cada valor del mejor atributo.\n",
        "          for v in self.get_values(node.X, best):\n",
        "            X_i, Y_i = [], []\n",
        "            for i in range(len(node.X)):\n",
        "              if node.X[i][best] == v:\n",
        "                x = node.X[i].copy()\n",
        "                x[best] = None\n",
        "                X_i.append(x)\n",
        "                Y_i.append(node.Y[i]) \n",
        "\n",
        "            # Creamos un nuevo nodo hijo con un bloque de la particion de los ejemplos.\n",
        "            atr_types_i = node.atr_types.copy()\n",
        "            atr_types_i[best] = None\n",
        "            child = Node(node, X_i, Y_i, atr_types_i, self.mayoria(Y_i))\n",
        "            node.childs.append(child)\n",
        "            node.cond.append((best, v))\n",
        "            queue.append(child)\n",
        "        else:\n",
        "          # Particionamos los ejemplos en menor y mayor o igual que la divison obtenida.\n",
        "          X_M, X_m, Y_M, Y_m = [], [], [], []\n",
        "          for i in range(len(node.X)):\n",
        "            x = node.X[i].copy()\n",
        "            if node.X[i][best] < best_d:\n",
        "              X_m.append(x)\n",
        "              Y_m.append(node.Y[i]) \n",
        "            else:\n",
        "              X_M.append(x)\n",
        "              Y_M.append(node.Y[i]) \n",
        "\n",
        "          # Con esa particion creamos dos nuevos nodos.\n",
        "          atr_types_i = node.atr_types.copy()\n",
        "          child_m = Node(node, X_m, Y_m, atr_types_i, self.mayoria(Y_m))\n",
        "          child_M = Node(node, X_M, Y_M, atr_types_i, self.mayoria(Y_M))\n",
        "          node.childs.append(child_m)\n",
        "          node.childs.append(child_M)\n",
        "          node.cond.append((best, \"<\", best_d))\n",
        "          node.cond.append((best, \">=\", best_d))\n",
        "          queue.append(child_m)\n",
        "          queue.append(child_M)\n",
        "\n",
        "  def predict(self, x):\n",
        "    \"\"\" Predecimos la etiqueta de un patron recorriendo el arbol.\"\"\"\n",
        "\n",
        "    # Partimos de la raiz.\n",
        "    node_i = self.tree\n",
        "    x_i = x.copy()\n",
        "    # Mientras no estemos en una hoja\n",
        "    while not node_i.leaf:\n",
        "      # En caso contrario, verificamos cual condicion del nodo cumple el patron\n",
        "      # y lo enviamos al hijo correspondiente.\n",
        "      cond = False\n",
        "      for i, c in enumerate(node_i.cond):\n",
        "        if len(c) == 2:\n",
        "          if x_i[c[0]] == c[1]:\n",
        "            node_i = node_i.childs[i]\n",
        "            cond = True\n",
        "            break\n",
        "        elif (c[1] == \"<\" and x_i[c[0]] < c[2]) or \\\n",
        "          (c[1] == \">=\" and x_i[c[0]] >= c[2]):\n",
        "          node_i = node_i.childs[i]\n",
        "          cond = True\n",
        "          break\n",
        "\n",
        "      if not cond: return node_i.default\n",
        "    return node_i.value\n",
        "\n",
        "  def print_tree(self, node_i = None, level = 0, atr = None):\n",
        "    \"\"\" Retornamos una representacion del arbol. \"\"\"\n",
        "    if node_i == None: node_i = self.tree\n",
        "    if atr == None: atr_i = self.atr_name.copy()\n",
        "    else: atr_i = atr.copy()\n",
        "    \n",
        "    if node_i.leaf: \n",
        "      text = \" -> \" + str(node_i.value)\n",
        "    else:\n",
        "      best = node_i.cond[0][0]\n",
        "      if len(node_i.cond[0]) == 2: text = \"\\n\" + level*\"|  \" + \"_ \" + atr_i[best]\n",
        "      else: text = \"\\n\" + level*\"|  \" + \"_ \" + atr_i[best]\n",
        "      for i, c in enumerate(node_i.cond):\n",
        "        text += \"\\n\" + (level+1)*\"|  \" + \" * \" + str(c[1])\n",
        "        if len(c) == 3: text += str(c[2])\n",
        "        text += self.print_tree(node_i.childs[i], level+1, atr_i)\n",
        "      text += \"\\n\" + (level)*\"|  \" + \"|_\"\n",
        "    return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45V7rVV_9JUJ"
      },
      "source": [
        "### **Random Forests**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ak2BNbE9IPk"
      },
      "source": [
        "class RandomForest:\n",
        "  def __init__(self, X, Y, atr_types, atr_names, num_trees):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.atr_types = atr_types\n",
        "    self.atr_names = atr_names\n",
        "    # Numero de arboles a usar.\n",
        "    self.num_trees = num_trees\n",
        "    # Arboles\n",
        "    self.trees = []\n",
        "\n",
        "  def train(self, splits=-1):\n",
        "    \"\"\" Entrenamos los distintos arboles. \"\"\"\n",
        "\n",
        "    for i in range(self.num_trees):\n",
        "      # Escogemos los datos de entrenamiento de forma aleatoria\n",
        "      N = len(self.X)\n",
        "      X_i, Y_i = [], []\n",
        "      for _ in range(N):\n",
        "        k = randint(0, N-1)\n",
        "        X_i.append(self.X[k])\n",
        "        Y_i.append(self.Y[k])\n",
        "\n",
        "      # Escogemos los atributos a deshabilitar de forma aleatoria\n",
        "      N = len(self.atr_types)\n",
        "      M = int(sqrt(N))\n",
        "      atr = [j for j in range(M)]\n",
        "      atr_avail = [1]*N\n",
        "      for _ in range(M):\n",
        "        k = randint(0, len(atr)-1)\n",
        "        atr_avail[atr.pop(k)] = 0\n",
        "\n",
        "      # Escogemos uno de los criterios de forma aleatoria\n",
        "      criterios = [\"Gini\", \"Entropy\"]\n",
        "      c = criterios[randint(0,1)]\n",
        "\n",
        "      # Creamos un nuevo arbol\n",
        "      t = DecisionTree(X_i.copy(), Y_i.copy(), self.atr_types, self.atr_names, atr_avail.copy())\n",
        "      t.train(splits, c)\n",
        "      self.trees.append(t)\n",
        "      print(str(i) + \"-esimo Arbol de Decision entrenado!\")\n",
        "\n",
        "  def predict(self, x, trees = None):\n",
        "    \"\"\" Ponemos a los arboles a votar y la etiqueta con mas votos sera retornada. \"\"\"\n",
        "    if trees == None: trees = self.trees\n",
        "\n",
        "    dic = {}\n",
        "    for t in trees:\n",
        "      r = t.predict(x)\n",
        "      if not r in dic: dic[r] = 1\n",
        "      else: dic[r] += 1\n",
        "\n",
        "    max_v = 0\n",
        "    v = None\n",
        "    for d in dic:\n",
        "      if dic[d] > max_v:\n",
        "        max_v = dic[d]\n",
        "        v = d \n",
        "    return v\n",
        "\n",
        "  def OOB(self):\n",
        "    \"\"\" Verificamos la calidad del random forest usando el metodo Out Of Bag. \"\"\"\n",
        "    acc, N = 0, 0\n",
        "    for i, x in enumerate(self.X):\n",
        "      trees = []\n",
        "      for t in self.trees:\n",
        "        if not x in t.X: trees.append(t)\n",
        "      if len(trees) > 0:\n",
        "        N += 1\n",
        "        if self.predict(x, trees) == self.Y[i][0]: acc += 1\n",
        "\n",
        "    if N == 0: return -1\n",
        "    return acc/N"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ8AXDXU9eS0"
      },
      "source": [
        "## **Lectura de Datos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnsL1AhS9UVx"
      },
      "source": [
        "import csv\n",
        " \n",
        "with open('/content/Skyserver_SQL2_27_2018 6_51_39 PM.csv', newline='') as File:  \n",
        "    reader = csv.reader(File)\n",
        "    X, Y = [], []\n",
        "    names = True\n",
        "    for row in reader:\n",
        "        # Datos inutiles.\n",
        "        row.pop(0)\n",
        "        row.pop(8)\n",
        "        row.pop(10)\n",
        "        row.pop(14)\n",
        "        \n",
        "        if not names:\n",
        "            Y.append([row.pop(10)])\n",
        "            X.append([float(r) for r in row])\n",
        "        else:\n",
        "            row.pop(10)\n",
        "            # Guardamos los nombres de cada atributo.\n",
        "            atr_names = row\n",
        "            names = False\n",
        "\n",
        "# Separamos los datos en entrenamiento y de prueba.\n",
        "X_train, Y_train = X[:7500].copy(), Y[:7500].copy()\n",
        "X_test, Y_test = X[7500:].copy(), Y[7500:].copy()\n",
        "atr_types = [\"Cont\",\"Cont\",\"Cont\",\"Cont\",\"Cont\",\"Cont\",\"Cont\",\"Cont\",\"Catg\",\n",
        "             \"Cont\",\"Cont\",\"Cont\",\"Cont\"]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buSyoJJn_OgD"
      },
      "source": [
        "## **Entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNd0-Jyg_Qj2",
        "outputId": "58234112-d9fb-4e06-b634-ce876dfe0b7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "RF = RandomForest(X_train, Y_train, atr_types, atr_names, 36)\n",
        "RF.train(splits=90)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0-esimo Arbol de Decision entrenado!\n",
            "1-esimo Arbol de Decision entrenado!\n",
            "2-esimo Arbol de Decision entrenado!\n",
            "3-esimo Arbol de Decision entrenado!\n",
            "4-esimo Arbol de Decision entrenado!\n",
            "5-esimo Arbol de Decision entrenado!\n",
            "6-esimo Arbol de Decision entrenado!\n",
            "7-esimo Arbol de Decision entrenado!\n",
            "8-esimo Arbol de Decision entrenado!\n",
            "9-esimo Arbol de Decision entrenado!\n",
            "10-esimo Arbol de Decision entrenado!\n",
            "11-esimo Arbol de Decision entrenado!\n",
            "12-esimo Arbol de Decision entrenado!\n",
            "13-esimo Arbol de Decision entrenado!\n",
            "14-esimo Arbol de Decision entrenado!\n",
            "15-esimo Arbol de Decision entrenado!\n",
            "16-esimo Arbol de Decision entrenado!\n",
            "17-esimo Arbol de Decision entrenado!\n",
            "18-esimo Arbol de Decision entrenado!\n",
            "19-esimo Arbol de Decision entrenado!\n",
            "20-esimo Arbol de Decision entrenado!\n",
            "21-esimo Arbol de Decision entrenado!\n",
            "22-esimo Arbol de Decision entrenado!\n",
            "23-esimo Arbol de Decision entrenado!\n",
            "24-esimo Arbol de Decision entrenado!\n",
            "25-esimo Arbol de Decision entrenado!\n",
            "26-esimo Arbol de Decision entrenado!\n",
            "27-esimo Arbol de Decision entrenado!\n",
            "28-esimo Arbol de Decision entrenado!\n",
            "29-esimo Arbol de Decision entrenado!\n",
            "30-esimo Arbol de Decision entrenado!\n",
            "31-esimo Arbol de Decision entrenado!\n",
            "32-esimo Arbol de Decision entrenado!\n",
            "33-esimo Arbol de Decision entrenado!\n",
            "34-esimo Arbol de Decision entrenado!\n",
            "35-esimo Arbol de Decision entrenado!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOnJpuT6FNGO"
      },
      "source": [
        "## **Resultados**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw0PPs9mDHPB",
        "outputId": "e2bfc38a-248c-4730-f932-aecdebc066ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "acc_train = 0\n",
        "for i in range(len(X_train)):\n",
        "  r = RF.predict(X_train[i])\n",
        "  if r == Y_train[i][0]: acc_train += 1\n",
        "acc_train /= len(X_train)\n",
        "\n",
        "acc_test = 0\n",
        "for i in range(len(X_test)):\n",
        "  r = RF.predict(X_test[i])\n",
        "  if r == Y_test[i][0]: acc_test += 1\n",
        "acc_test /= len(X_test)\n",
        "\n",
        "oob = RF.OOB()\n",
        "\n",
        "print(\"Acc train: \", acc_train)\n",
        "print(\"Acc test: \", acc_test)\n",
        "print(\"Acc OOB: \", oob)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc train:  0.9996\n",
            "Acc test:  0.9844\n",
            "Acc OOB:  0.9897333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AVJOpj_FVBx"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}